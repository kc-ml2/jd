# ML2 Take-home project

## 1.Review, re-implement, and reproduce a research/development project.

### ðŸ’¡ What to do
* Pick one project that can demonstrate your interest and experience.
  * If you are interested in developing a new deep learning algorithm, it can be one of the papers from the list below or any other paper you like.
  * Or any project related to one of the following topics:
    * Computer system optimization.
    * Repurposing available hardwares and/or developing new hardwares for ML/DL acceleration.
    * Web frontend UI for ML/DL research and development.
    * Full-stack ML/DL development on web browsers as a platform using WebAssembly, WebGL, TypeScript, TensorFlow.js, etc.
    * ML DevOps.
* Review and summarize the project.
* Re-implement the project from scratch. 
  * Feel free to use any material as a reference as long as you cite it; just don't copy-and-paste othersâ€™ code snippets. 
  * And it is totally fine to downscale the original project, i.e. reduced number of layers and weights for a deep model, or simpler requirements for a software.
* Set your own target performance and present an experiment showing your work has achieved the desired objective.
  * You donâ€™t have to reproduce the exact result of the original project. Toy tasks defined in the original project are good candidates.
  
* **If you have a similar project that you've done befor and can show your interests, you can submit it :)**  

### ðŸ’¡ What to submit 
* A link to a GitHub (or a similar Git hosting service) repository containing a README.md that has
  * a concise but extensive review of the project,
  * a description of the content of the repo,
  * a documentation of how to run the code,
  * showcase examples,
  * and any other materials you want to describe regarding the repo.

#### List of deep learning papers

* Hindsight Experience Replay (2017), Andrychowicz et al.,  https://arxiv.org/abs/1707.01495 
* A simple neural network module for relational reasoning (2017), Santoro et al., https://arxiv.org/abs/1706.01427 
* Attention Is All You Need (2017), Vaswani et al., https://arxiv.org/abs/1706.03762 
* PoincarÃ© Embeddings for Learning Hierarchical Representations (2017), Nickel et al., https://arxiv.org/abs/1705.08039
* Semi-Supervised Classification with Graph Convolutional Networks (2016), Kipf et al., https://arxiv.org/abs/1609.02907
* Dueling Network Architecture for Deep Reinforcement learning (2015), Wang et al., https://arxiv.org/abs/1511.06581 
* Continuous control with deep reinforcement learning (2015), Lillicrap et al., https://arxiv.org/abs/1509.02971
* DRAW: A Recurrent Neural Network For Image Generation (2015), Gregor et al., https://arxiv.org/pdf/1502.04623.pdf
* Show, Attend and Tell: Neural Image Caption Generation with Visual Attention (2015), Xu et al., https://arxiv.org/abs/1502.03044

#### Suggestions for applicants who are new to deep learning
* Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (2015), Radford et al., https://arxiv.org/abs/1511.06434
* Show and Tell: A Neural Image Caption Generator (2014), Vinyals et al., https://arxiv.org/abs/1411.4555
* Neural turing machines (2014), A. Graves et al., https://arxiv.org/pdf/1410.5401
* Auto-Encoding Variational Bayes (2013), Kingma et al., https://arxiv.org/abs/1312.6114
* Playing atari with deep reinforcement learning (2013), V. Mnih et al., https://arxiv.org/abs/1312.5602 
* Visualizing and understanding convolutional networks (2014), M. Zeiler and R. Fergus, http://arxiv.org/pdf/1311.2901

## 2.Portfolio
### ðŸ’¡ Goal
* Prepare a 15-minute presentation for past experiences in research and/or development.

### ðŸ’¡ What to submit
* A slide deck for the presentation.
